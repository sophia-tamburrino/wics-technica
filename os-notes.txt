9/2
Syscall Mechanics
* User Mode:
   * Cannot access memory of kernel mode, protects it
* Kernel Mode:
   * Memory has to be different than user memory
* Dual mode execution: communicate between both 
   * Example: 
      * printf(“print me!”)
      * write(1, “print me!”)
      * Put syscall number for write(4), file descriptor (1), and pointer to “print me!” into registers
      * ecall: mode bit → 0
         * Change to kernel stack
      * Call address in syscall tbl at index 4
      * Execute write system call
      * sret: mode bit → 1
         * Restore application registers
* Ecall: special call that switches from user mode to kernel
* Use argument registers and save/store them when returning back to user level
   * Save into trap frame struct
* Load / restore the argument values from frame struct
* Ecall to go to kernel, sret to go to user


I/O Interactions
* Each device controller understands how to operate their device.
* Takes commands from the CPU 0 has control regs/buffers
* Device controller: translates CPU commands into device actions
* Device Driver: OS code that understands the layout of dev control registers, and how to use them to transfer dataProtocol: Get a byte of data from a device
   * I/O is from the device to local buffer or controller
   * CPU sets registers in controller with command to read
      * Ex: character from keyboard
   * CPU waits while data is moved from controller buffer to memory
   * Device controllers are far away from CPU
      * CPU Cycle: 1 os class
      * USB: 5 years
* Interrupts
   * Replace waiting with
   * Ask for byte, go back to business, get notification, get byte
* Transfer control to interrupt service routine (ISR)
   * Identified by address in interrupt vector
* Hardware interrupt operations must save IP/SP at the interruption
* After servicing the interrupt, CPU resumes execution at previously interrupted address
* Kstk: kernel stack
* Dangerous to use user-level stack for kernel computation
   * So, split up stack
   * Mode1: User level stack
   * Mode 2: when interrupt happens, switch to kernel-level stack


9/4
System Structure
* System structure - how different parts of software are separated from each other and communicate
* How does a system use dual mode and virtual address spaces?
* Implications on security, reliability, programming style and maintainability
MSDOS: No structure or protection
Monolithic System Structure
* Includes Unix/Windows/OSX  
Microkernel System Structure
  

   * Moves functionality from the kernel to user space
   * Communicatuon between user servers using inter-process communication (IPC)
   * Benefits:
   * Easier to add functionality
   * More reliable and secure (less code running in kernel mode)
   * Detriments: takes a lot of cycles


Virtual Machines
   * A virtual machine host (the kernel) provides an interface identical to the underlying bare hardware
   * Other guest kernels execute in usr-mode
   * The API for virtual machines is a copy of the machine!
   * Goes from a monolithic system ^^ to..
  

   * VM system call? Interrupt for a VM?
  

   * System call has HW Virtualization Support
   * Interrupt comes from machine/processor, into the hypervisor, into the virtual hardware and vm kernel


VM benefits?
   * Multiple OS share the same hardware
   * Legacy support: support existing code as well as isolation
   * So they are protected from each other
   * Coordination
   * Some sharing of files
   * Communicate from each other via networking
   * Consolidation of many low-resource use systems onto fewer busier systems
   * Good security tenants don’t trust each other


Are containers the same as in kubernetes clusters?


In lecture
   * CPU Interaction: Interrupts
   * Eliminate all inefficiencies
   * DMA Direct Memory Access
   * Instead of CPU doing while(), set up memory device to transfer data to. CPU doesn’t transfer!
   * Problem: How does the OS and device know where the memory buffers are located to use for transfers? What if there are multiple I/O requests requiring multiple buffers?
   * Caching problem? CPU see value in cache and not value in I/O.
   * DMA
   * CPU
   * Sets up (large) buffers in memory before I/O
   * Asks device controller to transfer into buffer
   * Receives single interrupt for whole buffer of data
   * Device Controller
   * When device I/O complete (transferred to controller’s local buffers), transfer/copy data directly into memory
   * Send interrupt when transfer complete
   * Avoids CPU work for data-movement
   * What if transferred data is always a single byte?
   * DMA RING: The idea
   * “Circular” – access item N?
   * Go back to 0
   * “Wrapping”, “modula arithmetic”
   * Logically looks like ring since it goes from end to start
   * Because of the ring, the device controller doesn’t take up any time doing work as it accesses the ring. Eliminates basically all time of CPU waiting for data.
   * Polling vs. Interrupts
   * Polling: kernel repeatedly checks status of I/O
   * Read a device controller register
   * Has an I/O request finished, or not?
   * If I/O request has completed, CPU reads it into memory
   * Frequency of polling impacts latency and throughput of I/O
   * So should we simply poll at the highest possible frequency?
   * Is polling ever better than interrupts?
   * System overhead: polling better
   * OH Example:
   * Polling is expensive tho– so interrupts are good for notifications.
   * For a lot of people (piling up at the door), let in a group instead of letting in one at a time
   * Polling has a frequency at which you pull. Once/minute etc. Interrupts is once per whenever 


9/9
Virtual Machines
   * A VM host (the kernel) provides an interface identical to the underlying hardware
   * Hardware that understands not only a dual mode bit, but a VMuser/VMkernel and user/kernel


9/11
VM benefits
   * Legacy support
   * Consolidation
   * Good security, isolation
VMs vs Microkernels
   * Is either a generalization of the other?
   * VM relies on a host operating system providing an illusion of an environment
   * Hardware provides illusion of this ^
   * A microkernel also uses the hardware. Can be used as a host operating system to run the VM
Containers
   * The difference between a container and a full-fledged VM is that all containers share the same kernel of the host system.
   * Fundamental Technique: namespace separation
   * Structure: monolithic
   * VMs for security, not containers
   * Provide interfaces and abstractions so they have different namespaces


Execution Abstraction: Processes
   * Core/processor/cpu: The idea that there is a set of registers, and there is some sequential execution associated with it. Executes a flow of controls through a program
   * 8 cores means 8 programs that can execute in parallel
   * Multi-core processors
   * OS should abstract and provide
   * Each app should have its own memory
   * Protection between these applications
   * An executable program
   * Kept in executable file
   * UNIX process: active entity that includes a loaded program
   * Registers
   * Execution stack
   * Heap
   * Data and text segments (code)
OS Support for process memory
   * OS uses hardware to provide virtual address space
   * Each process thinks that it all has memory, which is an OS abstraction
   * There is a hole from 0->N of inaccessible memory. It is because if you dereference null, you want to return a segmentation fault


9/16
Kernel vs. Process Memory
   * Kernel mapped into each AS
   * Kernel data-structure storage
   * Why no separate kernel AS?
   * How is the kernel protected?
   * Revisit system call flow?
   * Switching AS → same kernel mem
   * * Kernel’s code is present in every process, in order to make dual mode execution work
   * What's stopping user level execution from stomping on kernel code? 
   * Kernel memory has a special tag on it so that it is kernel memory.
   * Accessing kernel memory from user mode, it causes an exception. 


Process Control Block (PCB)
   * Kernel, per process, data structure:
   * CPU registers
   * Process state
   * Mem management info
   * CPU accounting info
   * File info
   * Struct proc 


Process Queues 
   * Process/job queue - every process
   * Scheduling runqueue - procs in ready state, waiting to execute
   * Device queues - rarely queue, processes waiting for I/O completion (interrupts)
   * Processes migrate between queues


9/18
Process Cooperation
   * fork/exit/wait provide simple cooperation
   * Need other means for process coordination
   * Can you think of situations where this would be useful?
   * Is all IPC via fork/wait?
Examples
   * Modern browsers
   * Every tab is a process communicating with each other
   * Web servers
   * Flask - http requests
   * Shell


II
   * Concurrency - execution order of two processes is not predetermined
   * Multiple concurrently executing apps
   * Coordination between I/O bound processes
   * Like video streaming
   * Process 0, then process 1 runs
   * Parallelism - on multi-processor systems, two processes can execute at the same time
   * How can a single application utilize multicore machines?
   * Want multi-core so at least one process can execute on one of the cores
   * Process 1 and process 2 run at the same time
   * Message passing
   * Process A copies to kernel by send syscall; switches virtual address spaces; now executes in process B; Process B copies data up into the buffer by recv syscall
   * How read and write works
   * Pipes
   * File descriptor endpoints
   * One read, one write
   * 😣
   * Inherited across fork


9/23
   * IPC Synchronization: Blocking OPS
   * Blocking/synchronous operations
   * Process put on process communication queue
   * Data transferred only when other process also sends or recvs
   * Then placed back into runqueue
   * Proc A: recv(m)
   * Kernel: remove from runqueue, placed on comm queue
   * Kernel: switch to B
   * Proc B: send(m)
   * Kernel: move A to runqueue
   * Kernel: later, switch to A
   * Nonblocking: asynchronous ops
   * Send and recv don’t block the process
9/30
   * Race conditions
   * Non deterministic accesses that leave a data structure in an inconsistent state (no sequential execution)
   * Producer/consumer problem


10/2
   * Review of Peterson’s Algorithm
   * Has mutual exclusion
   * Progress is true if flag[you] == false. That means you can just go into the critical section
   * turn = you ensures bounded wait, 
   * because another thread is turn = me, and skips the turn = you while loop and goes right into the critical section. When exiting and trying to go into the loop again, the other thread sees that flag[me] is true, and can allow that one to enter the critical section ensuring bounded wait and mutual exclusion
   * One thread that says you and other thread says you; (point to eachother
   * Bakery Algorithm
   * Two threads could end up with the same ticket, so
   * Use ID to use break ties
   * Thread 1 proceeds before thread 2 as 1<2
   * Every thread has a number
   * Bakery II
   * Shared data structures for n threads
   * Notation
   * (a,b) < (c,d) means if (a,c) || (a==c) & (b < d)


10/28 
CFS Tracking Exec
   * Track the weighted execution time of threads
   * we=exec/w
   * Runqueue data structure- linux uses red-black trees
   * Periodic Task model
   * Period- span of repetitive execution
   * WCET: worst case execution time
   * Deadline: of task is a period after its activation
   * System is T={t1, t2, t3….ti}
   * Priority
   * Lower period, higher priority
   * Schedulability: schedulable if system meets all its deadlines
   * RTA response time analysis
   * Assumes fixed priority scheduling
   * RTAi(wn)=summation, ceiling of RTAi(wn-1)/pj, times ej


Stack/mem 10/30
   * Stack- grows down
   * What manages the stack?
   * Expandable memory-> page-fault below stack = alloc
   * Heap
   * Malloc, free - how are these implemented?
   * In C library, calls sbrk
   * How do you write malloc and free
   * Free_list data structure
   * Int size
   * Struct free_list *next
   * Free(mem)
   * Allocation Algorithms
   * Given a freelist
   * First fit - allocate the first hole that is big enough
   * Best fit - allocate the hole that results in the smallest hole after allocation
   * HW5: power of 2 allocator


11/11
   * Address Translation Scheme: 
   * Virtual addresses generated by CPU divided into
   * Page number (p) - most significant bits, to the left
   * Page offset (d) - least significant bits, to the right
   * MMU
   * Example: 8 byte pages
   * 2^3 = 8, so 3 bits represent < 8
   * 13 = msb - 01|101 - lsb
   * 101 = 5 = offset into page
   * 1 = page number (2nd page)
   * 26 = 11|010
   * 010 - offset of 2
   * 11 = 3 = 4th page
   * C code for getting page/offset
   * Page Tables: Inaccessible memory
   * Holes in virtual memory?
   * Inaccessible memory
   * Valid/invalid bit
   * Other bits in the PT
   * Readable
   * Writeable (COW)(copy on write)
   * Executable


11/13
   * Translation Lookaside Buffer (TLB)
   * Cache holding N page - > frame mappings
   * Associative memory, part of MMU
   * Hierarchical page Tables
   * Similar to prefix tries.
   * Will take the most significant bits of a virtual address space and find a node that points to another node in another page table, etc. until finding a physical address space
   * File System API
   *